{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents with Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader # Use DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "transcript_directory = \"youtube_transcripts\" # Directory containing your .txt files\n",
    "faiss_index_path = \"faiss_youtube_index\" # Path to save/load the index\n",
    "\n",
    "\n",
    "print(f\"Loading documents from: {transcript_directory}\")\n",
    "\n",
    "# It loads all .txt files by default in the specified directory\n",
    "loader = DirectoryLoader(\n",
    "    transcript_directory, \n",
    "    glob=\"*.txt\",       # Pattern to match files\n",
    "    loader_cls=TextLoader, # Specify loader for .txt\n",
    "    loader_kwargs={'encoding': 'utf8'}, # Ensure correct encoding\n",
    "    show_progress=True, # Show a progress bar\n",
    "    use_multithreading=True # Speed up loading (optional)\n",
    ")\n",
    "try:\n",
    "    documents = loader.load()\n",
    "    if not documents:\n",
    "        print(f\"Error: No documents found in '{transcript_directory}'. Please check the path and ensure .txt files exist.\")   \n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "    # Example: Check metadata of the first document\n",
    "    if documents:\n",
    "        print(\"Metadata example (first doc):\", documents[0].metadata) \n",
    "    # Tag documents with category\n",
    "    tagged_documents = []\n",
    "    for doc in documents:\n",
    "        file_name = doc.metadata.get(\"source\", \"\")\n",
    "        category = os.path.basename(file_name).replace(\".txt\", \"\")\n",
    "        doc.metadata[\"category\"] = category\n",
    "        tagged_documents.append(doc)\n",
    "\n",
    "    documents = tagged_documents\n",
    "except Exception as e:\n",
    "    print(f\"Error loading documents: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting documents into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(split_docs)} chunks.\")\n",
    "# Example: Check metadata of the first chunk (should match parent doc's metadata)\n",
    "if split_docs:\n",
    "     print(\"Metadata example (first chunk):\", split_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing embedding model...\")\n",
    "# Ensure API key is loaded or handled securely\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "if not openai_api_key:\n",
    "    print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
    "    # exit()\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Save FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating FAISS index from {len(split_docs)} chunks...\")\n",
    "# This will embed the documents and build the index\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "print(\"FAISS index created.\")\n",
    "\n",
    "print(f\"Saving FAISS index to: {faiss_index_path}\")\n",
    "vectorstore.save_local(faiss_index_path)\n",
    "print(\"FAISS index saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Use the Saved Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading and Testing Saved Index ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Loading and Testing Saved Index ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m.path.exists(faiss_index_path):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# IMPORTANT: Must use the *same* embedding function to load\u001b[39;00m\n\u001b[32m      4\u001b[39m     loaded_vectorstore = FAISS.load_local(faiss_index_path, embedding_model, allow_dangerous_deserialization=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Add allow_dangerous_deserialization\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFAISS index loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Loading and Testing Saved Index ---\")\n",
    "if os.path.exists(faiss_index_path):\n",
    "    # IMPORTANT: Must use the *same* embedding function to load\n",
    "    loaded_vectorstore = FAISS.load_local(faiss_index_path, embedding_model, allow_dangerous_deserialization=True) # Add allow_dangerous_deserialization\n",
    "    print(\"FAISS index loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Saved index path '{faiss_index_path}' not found for loading test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-4o-mini',  # Ensure this model is supported\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# conversational memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "print(\"Creating RetrievalQA chain...\")\n",
    "# Create a retriever from the loaded FAISS vector store\n",
    "faiss_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", # Or \"mmr\", etc.\n",
    "    search_kwargs={'k': 5}     # Number of documents to retrieve\n",
    ")\n",
    "\n",
    "# Initialize the QA chain with the FAISS retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # Or \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "    retriever=faiss_retriever,\n",
    "    return_source_documents=True # Optional: To see which chunks were retrieved\n",
    ")\n",
    "print(\"RetrievalQA chain created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Safely evaluate a basic school-level math expression like '2 + 3 * 4'.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "@tool\n",
    "def document_search(query: str) -> str:\n",
    "    \"\"\"Searches the local vector store for relevant documents based on the query.\"\"\"\n",
    "    # Use the existing RetrievalQA chain\n",
    "    response = qa_chain.invoke(query)\n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def image_search(query: str) -> str:\n",
    "    \"\"\"Search for a safe, educational image to help explain the topic visually.\"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.images(query, max_results=3, safesearch=\"moderate\")\n",
    "            if results:\n",
    "                for item in results:\n",
    "                    image_url = item['image']  # ✅ define it before using\n",
    "                    response = requests.head(image_url, allow_redirects=True, timeout=5)\n",
    "                    if response.status_code == 200:\n",
    "                        return f\"![Visual Aid]({image_url})\"\n",
    "    except Exception as e:\n",
    "        return f\"Image search failed: {str(e)}\"\n",
    "\n",
    "    return \"No relevant image found.\"\n",
    "\n",
    "tools = [calculator, document_search, image_search]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create agent with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "tools_str = \", \".join([tool.name for tool in tools])\n",
    "\n",
    "system_prompt = f'''\n",
    "You are a warm, friendly, and funny assistant designed to help parents teach educational topics to their children.\n",
    "\n",
    "You have access to the following tools: {tools_str}\n",
    "\n",
    "**Key Objectives**:\n",
    "- You **must** use the **Document Search** tool first.\n",
    "- Immediately after, you **must** call the **Image Search** tool to find an image,a diagram that supports the explanation.\n",
    "- These two tools are always used together, in that order, unless the topic is purely mathematical.\n",
    "\n",
    "- Use the **Calculator** only for math-related questions.\n",
    "- Use language that is **clear, fun, and child-friendly** so parents can easily explain topics to kids aged 6–10.\n",
    "\n",
    "**If Document Search returns \"No result found\"**, respond with:\n",
    "\"I couldn't find any direct information about {{input}}, but maybe this helps:\" — then give your best general answer.\n",
    "\n",
    "**Important**:\n",
    "- Use tools before giving your final answer.\n",
    "- Use kid-friendly metaphors like:  \n",
    "  - “Imagine your body is like a machine...”  \n",
    "  - “It’s kind of like when you...”  \n",
    "  - “Let’s pretend...”\n",
    "- Always include a visual (image or fallback text).\n",
    "- Your final answer should be short and written in regular adult language.\n",
    "\n",
    "**Language Behavior**:\n",
    "Respond in English by default, but if the user's question is in another language, reply only in that language.\n",
    "\n",
    "---\n",
    "\n",
    "Use the following format for every interaction:\n",
    "\n",
    "Question: {{input}}\n",
    "Thought: Think about which tool to use first (always start with Document Search).\n",
    "Action: Select one of [{tools_str}]\n",
    "Action Input: Write the query you are sending to the tool based on the input\n",
    "Observation: The result returned by the tool\n",
    "\n",
    "...(repeat Thought/Action/Observation as needed — **Document Search first, then Image Search**)...\n",
    "\n",
    "Thought: I now know the final answer\n",
    "\n",
    "Final Answer:\n",
    "[Category]\n",
    "\n",
    "Let me check the educational materials...\n",
    "\n",
    "**Explanation:**\n",
    "- [Explain using info from Document Search, in kids' language]\n",
    "- [Use examples, simple words, metaphors]\n",
    "- [Explain math steps if relevant]\n",
    "\n",
    "Let me find an image to help explain this visually...\n",
    "\n",
    "**Visual Aid:**\n",
    "![Visual Aid](image_url_here)  \n",
    "(or write: \"No relevant image or diagram was found.\")\n",
    "\n",
    "**Answer:**\n",
    "[A short, direct summary in adult language]\n",
    "\n",
    "---\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {{input}}\n",
    "Thought: {{agent_scratchpad}}\n",
    "'''\n",
    "\n",
    "\n",
    "custom_prompt = ChatPromptTemplate.from_template(system_prompt) #alternative to ChatPromptTemplate.from_messages thatWhat is the periodic table used for?What is the periodic table used for?\n",
    "\n",
    "# Use same ChatOpenAI model\n",
    "\n",
    "agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=custom_prompt) \n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = agent_executor.invoke({\"input\": \"Types of planets?\"})\n",
    "#print(response[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
